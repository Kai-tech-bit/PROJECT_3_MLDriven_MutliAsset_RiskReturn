{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbd3db1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Overview\n",
    "This project aims to build a cutting-edge, machine learning-powered framework for asset return forecasting, risk prediction, and portfolio optimization. Combining rigorous quantitative finance techniques with modern data science workflows, the system will leverage market data and advanced ML models to enhance risk-adjusted returns and demonstrate state-of-the-art portfolio management analytics.\n",
    "___\n",
    "\n",
    "### Objectives\n",
    "1) Predict asset returns or volatility using machine learning techniques\n",
    "2) Engineer informative financial features from historical price, volume, and macroeconomic data\n",
    "3) Construct and evaluate robust, data-driven portfolio allocation strategies\n",
    "4) Quantify and visualize portfolio risks using industry-standard metrics (e.g., VaR, Sharpe ratio, drawdown)\n",
    "5) Provide fully reproducible, commented code and a comprehensive walkthrough of all methodologies.\n",
    "___\n",
    "\n",
    "###  Workflow\n",
    "1) Data Collection: Source and preprocess historical market and macroeconomic data for selected assets.\n",
    "2) Exploratory Data Analysis: Investigate statistical properties, visualize trends, and identify key drivers.\n",
    "3) Feature Engineering: Extract and construct relevant features (technical indicators, rolling stats, etc.)\n",
    "\n",
    "##### Machine Learning Modeling:\n",
    "\n",
    "1) Select regression or classification models (Linear Regression, Random Forest, XGBoost/LSTM, etc.)\n",
    "2) Tune and train models for forecasting returns/volatility.\n",
    "\n",
    "##### Backtesting and Evaluation:\n",
    "1) Simulate predictions and portfolio allocation over out-of-sample test sets.\n",
    "2) Assess performance using financial metrics and realistic constraints.\n",
    "\n",
    "##### Portfolio Optimization:\n",
    "1) Implement optimization routines to construct portfolios that maximize risk-adjusted returns.\n",
    "2) Integrate ML predictions into allocation strategies.\n",
    "\n",
    "##### Visualization and Reporting:\n",
    "1) Generate plots (prediction vs. actual, cumulative returns, feature importance)\n",
    "2) Summarize key results and business implications\n",
    "\n",
    "##### Documentation:\n",
    "Ensure all code sections and methodology are clearly explained for full reproducibility and understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757daa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Nessecary Libraries\n",
    "\n",
    "# Data handling and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and modeling\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Advanced models (if needed later)\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# For feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For saving/loading models and saving folders\n",
    "import pickle\n",
    "import joblib as jb\n",
    "import os \n",
    "\n",
    "# For downloading data\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "\n",
    "os.makedirs('Results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee26c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 gathering the relevant data and processing it for the model training and developement \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Define tickers and date range\n",
    "tickers = ['MSFT','AAPL','SPY','AMZN','TSLA','NVDA']\n",
    "data_frames = []\n",
    "start_date = datetime(2010, 1, 1)  \n",
    "end_date = datetime.today()\n",
    "\n",
    "for ticker in tickers:\n",
    "    df = pdr.get_data_stooq(ticker, start=start_date, end=end_date)  # OHLCV data\n",
    "    df.columns = pd.MultiIndex.from_product([[ticker], df.columns])    # multi-level columns with ticker\n",
    "    data_frames.append(df)\n",
    "\n",
    "# Concatenate all ticker DataFrames on columns to form a multi-level DataFrame\n",
    "Data = pd.concat(data_frames, axis=1).sort_index(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# checking mssing values per ticker columns and interpoladin them to get the data continoius \n",
    "null_cols =[]\n",
    "for ticker in Data.columns:\n",
    "    if Data[ticker].isna().sum() > 0:\n",
    "        null_cols.append(ticker)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "null_cols\n",
    "\n",
    "# fitting and filling the missing values using interpolation\n",
    "Data.loc[:, null_cols] = Data.loc[:, null_cols].interpolate(method='time',degree=1).ffill().bfill()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOw perfoming the EDA and Feature Engineering to observe the trends and the important feaures\n",
    "\n",
    "# Feature Engineering \n",
    "Feature_list = ['Pct_returns','Log_Returns','Rolling_Mean_returns','Rolling_STD','Rolling_Skewness','Rolling_Kurtosis',\n",
    "                'SMA(20)','SMA(50)','EMA','MACD','RSI','BOLLINGER_BANDS',\n",
    "                'Day_of_Week','Month','Year','Week_of_Year','Day_of_Year','Weekend','Holiday',\n",
    "                'Rolling_Correlations'\n",
    "                ]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -1 * delta.clip(upper=0)\n",
    "    avg_gain = gain.rolling(window=window).mean()\n",
    "    avg_loss = loss.rolling(window=window).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def compute_macd(series, fast=12, slow=26, signal=9):\n",
    "    ema_fast = series.ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = series.ewm(span=slow, adjust=False).mean()\n",
    "    macd = ema_fast - ema_slow\n",
    "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
    "    histogram = macd - signal_line\n",
    "    return macd, signal_line, histogram\n",
    "\n",
    "def compute_bollinger_bands(series, window=20):\n",
    "    sma = series.rolling(window=window).mean()\n",
    "    std = series.rolling(window=window).std()\n",
    "    upper_band = sma + 2 * std\n",
    "    lower_band = sma - 2 * std\n",
    "    return upper_band, lower_band\n",
    "\n",
    "technical_features = {}\n",
    "\n",
    "for ticker in Data.columns.get_level_values(0).unique():\n",
    "    close = Data[ticker]['Close']\n",
    "    high = Data[ticker]['High']\n",
    "    low = Data[ticker]['Low']\n",
    "    volume = Data[ticker]['Volume']\n",
    "    Data[(ticker,'log_returns')] = np.log(Data[ticker]['Close'] / Data[ticker]['Close'].shift(1))\n",
    "    Data[(ticker,'Pct_change')] = (Data[ticker]['Close'] - Data[ticker]['Close'].shift(1)) / Data[ticker]['Close'].shift(1)\n",
    "    Data[(ticker,'rolling_logvol_30')] = Data[(ticker,'log_returns')].rolling(window=30).std()\n",
    "    \n",
    "    # RSI (14-day)\n",
    "    technical_features[(ticker, 'RSI_14')] = compute_rsi(close)\n",
    "    \n",
    "    # MACD\n",
    "    macd, signal_line, hist = compute_macd(close)\n",
    "    technical_features[(ticker, 'MACD')] = macd\n",
    "    technical_features[(ticker, 'MACD_Signal')] = signal_line\n",
    "    technical_features[(ticker, 'MACD_Histogram')] = hist\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    upper_band, lower_band = compute_bollinger_bands(close)\n",
    "    technical_features[(ticker, 'Bollinger_Upper')] = upper_band\n",
    "    technical_features[(ticker, 'Bollinger_Lower')] = lower_band\n",
    "    \n",
    "    # SMA and EMA\n",
    "    technical_features[(ticker, 'SMA_20')] = close.rolling(window=20).mean()\n",
    "    technical_features[(ticker, 'EMA_20')] = close.ewm(span=20, adjust=False).mean()\n",
    "    \n",
    "    # Average True Range (ATR) - volatility measure\n",
    "    tr1 = high - low\n",
    "    tr2 = (high - close.shift(1)).abs()\n",
    "    tr3 = (low - close.shift(1)).abs()\n",
    "    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    atr = true_range.rolling(window=14).mean()\n",
    "    technical_features[(ticker, 'ATR_14')] = atr\n",
    "    \n",
    "    # On Balance Volume (OBV) - volume momentum\n",
    "    direction = np.sign(close.diff())\n",
    "    obv = (direction * volume).fillna(0).cumsum()\n",
    "    technical_features[(ticker, 'OBV')] = obv\n",
    "\n",
    "# Combine all features into a DataFrame\n",
    "tech_indicators_df = pd.DataFrame(technical_features)\n",
    "\n",
    "# Sort columns by ticker and feature\n",
    "tech_indicators_df = tech_indicators_df.sort_index(axis=1, level=[0, 1])\n",
    "\n",
    "Data.sort_index(axis=1, level=[0, 1], inplace=True)\n",
    "\n",
    "Final_Data = pd.concat([Data, tech_indicators_df], axis=1).sort_index(axis=1, level=[0, 1])\n",
    "\n",
    "Final_Data.to_csv('Results/Final_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do EDA and then feature slection for our model that is what arr the iportant fetaure for our model\n",
    "\n",
    "# lets have pair plots for the Data frame to get the important relaton between features\n",
    "\n",
    "\n",
    "for ticker in tickers:\n",
    "    sns.pairplot(Data[ticker], diag_kind='kde',title=f'Pairplot for {ticker}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Results/{ticker}_pairplot.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff8a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for each ticker for their feataure vs log_returns\n",
    "corr_matrix_dict_log_returns={}\n",
    "corr_matrix_dict_rolling_logvol_30={}\n",
    "for ticker in tickers:\n",
    "    corr_matrix_dict_log_returns[ticker] =Final_Data[ticker].drop(columns=['log_returns','Pct_change']).dropna().corrwith(Final_Data[(ticker,'log_returns')]).sort_values(ascending=False)\n",
    "    corr_matrix_dict_rolling_logvol_30[ticker] =Final_Data[ticker].drop(columns=['log_returns','rolling_logvol_30']).dropna().corrwith(Final_Data[(ticker,'rolling_logvol_30')]).sort_values(ascending=False)\n",
    "    \n",
    "    \n",
    "# Impoetant top 10 fetaures for the log_returns and rolling vol_30 for each ticker\n",
    "Features_log_returns ={}\n",
    "Features_rolling_logvol_30 ={}\n",
    "for ticker in tickers:\n",
    "    Features_log_returns[ticker] = list(corr_matrix_dict_log_returns[ticker].head(10).index)\n",
    "    Features_rolling_logvol_30[ticker] = list(corr_matrix_dict_rolling_logvol_30[ticker].head(10).index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6fc1c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating final data fro each ticker for Each tikcer and their target and rolling vol dt fames to finlay use in our train and test and model \n",
    "\n",
    "Model_AApl_ret = pd.concat([Final_Data['AAPL'][Features_log_returns['AAPL']],Final_Data['AAPL']['log_returns'].shift(-1)],axis=1).dropna()\n",
    "Model_MSFT_ret = pd.concat([Final_Data['MSFT'][Features_log_returns['MSFT']],Final_Data['MSFT']['log_returns'].shift(-1)],axis=1).dropna()\n",
    "Model_AMZN_ret = pd.concat([Final_Data['AMZN'][Features_log_returns['AMZN']],Final_Data['AMZN']['log_returns'].shift(-1)],axis=1).dropna()\n",
    "Model_SPY_ret = pd.concat([Final_Data['SPY'][Features_log_returns['SPY']],Final_Data['SPY']['log_returns'].shift(-1)],axis=1).dropna()\n",
    "Model_TSLA_ret = pd.concat([Final_Data['TSLA'][Features_log_returns['TSLA']],Final_Data['TSLA']['log_returns'].shift(-1)],axis=1).dropna()\n",
    "Model_NVDA_ret = pd.concat([Final_Data['NVDA'][Features_log_returns['NVDA']],Final_Data['NVDA']['log_returns'].shift(-1)],axis=1).dropna()\n",
    "\n",
    "Model_AApl_roll = pd.concat([Final_Data['AAPL'].loc[Final_Data['AAPL']['rolling_logvol_30'].notna()][Features_rolling_logvol_30['AAPL']],Final_Data['AAPL']['rolling_logvol_30'].shift(-1)],axis=1).dropna()\n",
    "Model_MSFT_roll = pd.concat([Final_Data['MSFT'].loc[Final_Data['MSFT']['rolling_logvol_30'].notna()][Features_rolling_logvol_30['MSFT']],Final_Data['MSFT']['rolling_logvol_30'].shift(-1)],axis=1).dropna()\n",
    "Model_AMZN_roll = pd.concat([Final_Data['AMZN'].loc[Final_Data['AMZN']['rolling_logvol_30'].notna()][Features_rolling_logvol_30['AMZN']],Final_Data['AMZN']['rolling_logvol_30'].shift(-1)],axis=1).dropna()    \n",
    "Model_SPY_roll = pd.concat([Final_Data['SPY'].loc[Final_Data['SPY']['rolling_logvol_30'].notna()][Features_rolling_logvol_30['SPY']],Final_Data['SPY']['rolling_logvol_30'].shift(-1)],axis=1).dropna() \n",
    "Model_TSLA_roll = pd.concat([Final_Data['TSLA'].loc[Final_Data['TSLA']['rolling_logvol_30'].notna()][Features_rolling_logvol_30['TSLA']],Final_Data['TSLA']['rolling_logvol_30'].shift(-1)],axis=1).dropna()    \n",
    "Model_NVDA_roll = pd.concat([Final_Data['NVDA'].loc[Final_Data['NVDA']['rolling_logvol_30'].notna()][Features_rolling_logvol_30['NVDA']],Final_Data['NVDA']['rolling_logvol_30'].shift(-1)],axis=1).dropna()    \n",
    "\n",
    "os.makedirs('Results/Model_Data', exist_ok=True)\n",
    "\n",
    "for ticker in tickers:\n",
    "    Model_AApl_ret.to_csv(f'Results/Model_Data/Model_AApl_ret.csv')\n",
    "    Model_MSFT_ret.to_csv(f'Results/Model_Data/Model_MSFT_ret.csv')\n",
    "    Model_AMZN_ret.to_csv(f'Results/Model_Data/Model_AMZN_ret.csv')\n",
    "    Model_SPY_ret.to_csv(f'Results/Model_Data/Model_SPY_ret.csv')\n",
    "    Model_TSLA_ret.to_csv(f'Results/Model_Data/Model_TSLA_ret.csv')\n",
    "    Model_NVDA_ret.to_csv(f'Results/Model_Data/Model_NVDA_ret.csv')\n",
    "    Model_AApl_roll.to_csv(f'Results/Model_Data/Model_AApl_roll.csv')\n",
    "    Model_MSFT_roll.to_csv(f'Results/Model_Data/Model_MSFT_roll.csv')\n",
    "    Model_AMZN_roll.to_csv(f'Results/Model_Data/Model_AMZN_roll.csv')\n",
    "    Model_SPY_roll.to_csv(f'Results/Model_Data/Model_SPY_roll.csv')\n",
    "    Model_TSLA_roll.to_csv(f'Results/Model_Data/Model_TSLA_roll.csv')\n",
    "    Model_NVDA_roll.to_csv(f'Results/Model_Data/Model_NVDA_roll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1820feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we perfomr the machine leaning part where we will create pipe line for each ticker and have model slected for each ticker\n",
    "we will use the metrics to score the outcoes of our model\n",
    "we will use liner regressor and randomforest regressor \n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
